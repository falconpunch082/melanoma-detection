{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "397da3d7-db79-4497-9200-5fc9b941a6c4",
   "metadata": {},
   "source": [
    "# Model testing\n",
    "\n",
    "The purpose of this notebook is to determine which CNN architecture is the most efficient yet accurate\n",
    "one for the purposes of determining whether a photo provided by the enduser shows signs of skin cancer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffbef4cf-d8ea-4dc8-8037-afa39a9459a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dalet\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importing relevant packages\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization, Conv2D, MaxPool2D, Lambda\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "import skimage\n",
    "from skimage import io, color\n",
    "from skimage.filters import threshold_otsu\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e2bc6b4-69b8-47ce-8e4b-2c60755858c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 42639 files belonging to 2 classes.\n",
      "Found 5329 files belonging to 2 classes.\n",
      "Found 5332 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Assigning image dataset into variables for later use\n",
    "# The image_dataset_from_directory function automatically assigns an image to its respective class\n",
    "# i.e., if an image is part of the '1' file (meaning it shows skin with skin cancer),\n",
    "# the photo is automatically flagged as showing skin cancer.\n",
    "train_dataset = tf.keras.preprocessing.image_dataset_from_directory('img_data/train', image_size=(300,300), batch_size=32)\n",
    "val_dataset = tf.keras.preprocessing.image_dataset_from_directory('img_data/val', image_size=(300,300), batch_size=32)\n",
    "test_dataset = tf.keras.preprocessing.image_dataset_from_directory('img_data/test', image_size=(300,300), batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefb03fb-877f-4903-ad48-cf51de33a55e",
   "metadata": {},
   "source": [
    "## Creating the preprocessing custom layer\n",
    "\n",
    "This layer will:\n",
    "- first reshape the provided pictures into a set dimension for uniformity,\n",
    "- then save the cropped picture in a variable for later use\n",
    "- meanwhile, another copy of the cropped picture is made and greyscaled\n",
    "- the greyscaled photo is then used for otsu thresholding which is used for masking\n",
    "- finally, the processed photo used by the model is made by overlaying the mask on the\n",
    "   previously saved cropped picture to create a photo with only the important features\n",
    "   of the picture (i.e., just the skinspot, disregarding healthy skin and hair).\n",
    "\n",
    "Through preprocessing, it is hoped that the model's accuracy and speed will improve due to the removal of\n",
    "unnecessary features like image size and the presence of healthy skin and hair.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03b4ab87-2b08-4e9c-b592-afa072a248eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class prepro(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(prepro, self).__init__()\n",
    "  \n",
    "    def build(self, input_shape):\n",
    "        pass\n",
    "    \n",
    "    def call(self, img):\n",
    "\n",
    "        # Resize the image to a consistent size\n",
    "        img = img.resize((300, 300))\n",
    "        \n",
    "        # Convert the image to a NumPy array\n",
    "        img_array = np.array(img)\n",
    "        \n",
    "        # Normalize pixel values to be between 0 and 1\n",
    "        img_array = img_array.astype('float32') / 299\n",
    "        \n",
    "        # Converting RGB picture to greyscale for thresholding\n",
    "        img_gc = color.rgb2gray(img)\n",
    "    \n",
    "        # Global thresholding with Otsu\n",
    "        thresh = threshold_otsu(img_gc)\n",
    "        img_t = img_gc <= thresh\n",
    "    \n",
    "        # Creating mask using threshold image\n",
    "        # Value 0 as black and white photo used\n",
    "        mask = np.where(img_t >= 0, img_t, 0)\n",
    "    \n",
    "        # Overlaying mask on original image\n",
    "        # Nested for loop for each 'row' of img\n",
    "        for h in range(mask.shape[0]):\n",
    "          # For each 'column' of img\n",
    "            for w in range(mask.shape[1]):\n",
    "              # If the pixel chosen from the mask is white, add in the pixel from the original image\n",
    "              # Otherwise, discard/make pixel black\n",
    "                if mask[h][w] == 0:\n",
    "                    for i in range(3):\n",
    "                        img[h][w][i] = 0\n",
    "                else:\n",
    "                    continue\n",
    "            \n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fa458f0-2a81-44f5-aa1e-289c5a490147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining preprocessing layer\n",
    "def prepro(img):\n",
    "    # Resize the image to a consistent size\n",
    "    img = img.resize((300, 300))\n",
    "    \n",
    "    # Convert the image to a NumPy array\n",
    "    img_array = np.array(img)\n",
    "    \n",
    "    # Normalize pixel values to be between 0 and 1\n",
    "    img_array = img_array.astype('float32') / 299\n",
    "    \n",
    "    # Converting RGB picture to greyscale for thresholding\n",
    "    img_gc = color.rgb2gray(img)\n",
    "\n",
    "    # Global thresholding with Otsu\n",
    "    thresh = threshold_otsu(img_gc)\n",
    "    img_t = img_gc <= thresh\n",
    "\n",
    "    # Creating mask using threshold image\n",
    "    # Value 0 as black and white photo used\n",
    "    mask = np.where(img_t >= 0, img_t, 0)\n",
    "\n",
    "    # Overlaying mask on original image\n",
    "    # Nested for loop for each 'row' of img\n",
    "    for h in range(mask.shape[0]):\n",
    "      # For each 'column' of img\n",
    "        for w in range(mask.shape[1]):\n",
    "          # If the pixel chosen from the mask is white, add in the pixel from the original image\n",
    "          # Otherwise, discard/make pixel black\n",
    "            if mask[h][w] == 0:\n",
    "                for i in range(3):\n",
    "                    img[h][w][i] = 0\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    # Function output - processed image\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dcded8-cef9-41ec-b0e3-5b5c82936f6a",
   "metadata": {},
   "source": [
    "## Creating initial CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1f4c091-2549-46e9-9212-75ca968465cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dalet\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\dalet\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating Sequential object\n",
    "model_1 = Sequential()\n",
    "\n",
    "# Adding layers to the Sequential object\n",
    "#model_1.add(keras.Input(shape=(1,)))\n",
    "#model_1.add(Lambda(lambda img: prepro(img)))\n",
    "model_1.add(Conv2D(32, kernel_size=(3,3), activation='relu', padding='same', input_shape=(900,3))) # Start of neural network\n",
    "model_1.add(Conv2D(64, (3,3), activation='relu'))\n",
    "model_1.add(MaxPool2D(pool_size=(2,2)))\n",
    "model_1.add(Dropout(0.25))\n",
    "model_1.add(Flatten())\n",
    "model_1.add(Dense(128, activation='relu'))\n",
    "model_1.add(Dropout(0.25))\n",
    "model_1.add(Dense(128, activation='relu'))\n",
    "model_1.add(Dropout(0.25))\n",
    "model_1.add(Dense(1, activation='sigmoid')) # Output layer - just one neuron because of binary classification (one or zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "964c0729-6cbf-44d5-975f-43a6cbb2257a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 300, 300, 32)      896       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 298, 298, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 149, 149, 64)      0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 149, 149, 64)      0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1420864)           0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               181870720 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 181906753 (693.92 MB)\n",
      "Trainable params: 181906753 (693.92 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Checking the structure of the model\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ec381a4-8987-4d70-b49c-ef50b36b9752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dalet\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compiling the model\n",
    "model_1.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy', Precision(), Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13a27657-c0ed-432b-bfe6-60c0d0eb530a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1333/1333 [==============================] - 4765s 4s/step - loss: 0.4029 - accuracy: 0.8627 - precision: 0.8493 - recall: 0.9853\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "fit_model_1 = model_1.fit(train_dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8fb59f5-4f5f-4d0e-910d-a793e44f1a06",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluate the model using the test data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model_loss, model_accuracy \u001b[38;5;241m=\u001b[39m model_1\u001b[38;5;241m.\u001b[39mevaluate(test_dataset,verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_accuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_1' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = model_1.evaluate(test_dataset,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a019723b-304d-4f6e-83ce-4fc8509aced9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export our model to HDF5 file\n",
    "model_1.save(\"model1.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
